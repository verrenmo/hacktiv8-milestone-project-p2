# ğŸš€ **Hacktiv8 - Milestone 3 Project**
**by Verren Monica**  
**Phase 2 | Batch: RMT-038**  

## ğŸ“Œ **Project Overview**  
This project automates the **Extract, Transform, and Load (ETL) process** using **Apache Airflow**. The pipeline extracts sales data from **PostgreSQL**, cleans and processes it, then loads it into **Elasticsearch**. The data is then visualized in **Kibana** to generate insights into:  

ğŸ“Š **Product category preferences**  
ğŸ›ï¸ **Customer behavior**  
â­ **Shopping experience evaluations**  

These insights will help **Supermarket ABC** formulate strategic recommendations to **boost sales and remain competitive** against e-commerce.  

## ğŸ“– **Background**  
Based on the sources I read, the trend of online shopping is currently experiencing rapid growth. The development of **e-commerce**, which offers the convenience of shopping from home, has become a significant concern for **supermarkets** to continuously improve their services and remain competitive.  

To stay ahead, supermarkets must understand key factors such as **customer behavior and sales trends** to refine their business strategies. By conducting this analysis, **valuable insights** can be obtained to support **data-driven decisions** that promote **business sustainability**.  

## ğŸ¯ **Objectives**  
As a **Data Analyst at Supermarket ABC**, my goal is to prepare a **comprehensive report** that provides insights into:  
âœ”ï¸ **Product category preferences**  
âœ”ï¸ **Customer behavior**  
âœ”ï¸ **Shopping experience evaluations**  

Additionally, I will provide **strategic recommendations** based on the findings to **increase sales and profits**, ensuring **Supermarket ABC can sustain and compete amidst intense competition from e-commerce**.  

## ğŸ“‚ **Dataset Source**  
ğŸ“Œ [Dataset Link](https://www.kaggle.com/datasets/aungpyaeap/supermarket-sales/data))

## âš™ï¸ **ETL Process (Airflow DAG)**  
This ETL process is designed to **automate data transfer from PostgreSQL to Elasticsearch**. The dataset contains **3 months of sales data** from Supermarket ABC.  
ğŸ”¹ **Step 1: Fetch from PostgreSQL** â€“ Extracts data and saves it as a `.csv` file.  
ğŸ”¹ **Step 2: Data Cleaning** â€“ Removes duplicates, normalizes column names, handles missing values, and adjusts date formats.  
ğŸ”¹ **Step 3: Post to Elasticsearch** â€“ Uploads the cleaned data for indexing and visualization.  

### ğŸ— **Technology Stack**  
âœ… **Docker** â€“ Containerized the entire pipeline for seamless execution.  
âœ… **Apache Airflow** â€“ Orchestrates the ETL workflow.  
âœ… **PostgreSQL** â€“ Stores raw sales data.  
âœ… **Elasticsearch** â€“ Stores processed data for querying and visualization.  
âœ… **Kibana** â€“ Creates dashboards and visual reports.  
âœ… **Great Expectations** â€“ Ensures data quality before ingestion.  

## ğŸ“œ **Scripts**  
ğŸ“Œ **[P2M3_verrenmonica_DAG.py](P2M3_verrenmonica_DAG.py)** â€“ Python script defining the **Airflow DAG** for the ETL process.  

## ğŸ“Š **Notebooks**  
ğŸ“Œ **[P2M3_verrenmonica_GX.ipynb](P2M3_verrenmonica_GX.ipynb)** â€“ Performs **data validation** on the cleaned dataset using **Great Expectations (GX)** to ensure data integrity before ingestion into Elasticsearch.  

## ğŸ“Œ **Conclusion & Business Impact**  
âœ… **Automated ETL workflow**, reducing manual data processing efforts.  
âœ… **High-quality data validation** using Great Expectations.  
âœ… **Actionable insights from sales data**, enabling better decision-making for Supermarket ABC.  
âœ… **Improved competitive edge** by leveraging data-driven strategies.  

## ğŸ”— **Let's Connect!**  
ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/verren-monica/) 

### âœ¨ **Changes & Improvements:**  
âœ” **Pisahkan DAG ke bagian "Scripts"** karena itu file **Python script (`.py`)**, bukan notebook.  
âœ” **Buat bagian "Notebooks" khusus untuk Great Expectations** agar lebih rapi.  
âœ” **Tetap menjaga flow yang profesional dan engaging.**  

Ini sudah clean & sesuai dengan workflow-mu. ğŸš€ Mau ada tambahan lagi? ğŸ˜Š
